"""
ì²´ê³„ì ì¸ ë¡œê¹… ì‹œìŠ¤í…œ
Phase 4 ë¦¬íŒ©í† ë§ì„ ìœ„í•œ ì²´ê³„ì ì¸ ë¡œê¹… ì‹œìŠ¤í…œ
"""

import sys
import os
import logging
import json
import time
from datetime import datetime, timedelta
import uuid
from typing import Dict, List, Optional, Any
from pathlib import Path
import threading
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class LoggingService:
    """ì²´ê³„ì ì¸ ë¡œê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        # ì‹¤í–‰ ê³ ìœ  ì‹ë³„ì (íƒ€ì„ìŠ¤íƒ¬í”„ + ì§§ì€ UUID)
        self.run_id = datetime.now().strftime('%Y%m%d_%H%M%S') + '_' + uuid.uuid4().hex[:6]
        self.performance_metrics = defaultdict(list)
        self.operation_history = []
        self.log_levels = {
            'DEBUG': logging.DEBUG,
            'INFO': logging.INFO,
            'WARNING': logging.WARNING,
            'ERROR': logging.ERROR,
            'CRITICAL': logging.CRITICAL
        }
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.setup_log_directories()
        
        # ì£¼ì˜: ì „ì—­ ë¡œê¹… ì´ˆê¸°í™”ì˜ ë‹¨ì¼ ì§„ì…ì ì€ app.py(dictConfig)ì…ë‹ˆë‹¤.
        # ì´ í´ë˜ìŠ¤ëŠ” ë³„ë„ íŒŒì¼ í•¸ë“¤ëŸ¬/í—¬í¼ë¥¼ ì œê³µí•˜ëŠ” ë³´ì¡° ì—­í• ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        # Deprecated: ì•„ë˜ ì „ì—­ ë¡œê¹… ì´ˆê¸°í™” í˜¸ì¶œì€ ë” ì´ìƒ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # self.setup_logging()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self.setup_performance_monitoring()
    
    def setup_log_directories(self):
        """ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •"""
        log_dirs = ['logs', 'logs/analysis', 'logs/performance', 'logs/errors']
        
        for log_dir in log_dirs:
            Path(log_dir).mkdir(parents=True, exist_ok=True)
    
    def setup_logging(self):
        """Deprecated: ì „ì—­ ë¡œê¹… ì´ˆê¸°í™”ëŠ” app.pyì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ ë¡œê±° ì„¤ì •
        run_app_log = f"logs/app_{self.run_id}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(run_app_log, encoding='utf-8', mode='w'),
                logging.StreamHandler()
            ]
        )
        
        # ë¶„ì„ ì „ìš© ë¡œê±°
        self.analysis_logger = self._create_logger(
            'analysis_logger',
            f"logs/analysis/analysis_{self.run_id}.log",
            logging.INFO
        )
        
        # ì„±ëŠ¥ ì „ìš© ë¡œê±°
        self.performance_logger = self._create_logger(
            'performance_logger',
            f"logs/performance/performance_{self.run_id}.log",
            logging.INFO
        )
        
        # ì—ëŸ¬ ì „ìš© ë¡œê±°
        self.error_logger = self._create_logger(
            'error_logger',
            'logs/errors/errors.log',
            logging.ERROR
        )
        
        # ì‹œìŠ¤í…œ ì „ìš© ë¡œê±°
        self.system_logger = self._create_logger(
            'system_logger',
            f"logs/system_{self.run_id}.log",
            logging.INFO
        )
    
    def setup_performance_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        self.performance_start_times = {}
        self.performance_metrics = defaultdict(list)
    
    def _create_logger(self, name: str, log_file: str, level: int) -> logging.Logger:
        """ë¡œê±° ìƒì„±"""
        logger = logging.getLogger(name)
        logger.setLevel(level)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(level)
        
        # í¬ë§·í„°
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        
        return logger
    
    def log_analysis_start(self, ticker: str, market_type: str, operation: str = 'analysis'):
        """ë¶„ì„ ì‹œì‘ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'ticker': ticker,
            'market_type': market_type,
            'status': 'started',
            'thread_id': threading.get_ident()
        }
        
        self.analysis_logger.info(f"ë¶„ì„ ì‹œì‘: {json.dumps(log_data, ensure_ascii=False)}")
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_start_times[f"{ticker}_{operation}"] = time.time()
    
    def log_analysis_complete(self, ticker: str, result: Dict, operation: str = 'analysis'):
        """ë¶„ì„ ì™„ë£Œ ë¡œê¹…"""
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = self.performance_start_times.get(f"{ticker}_{operation}")
        execution_time = time.time() - start_time if start_time else 0
        
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'ticker': ticker,
            'status': 'completed',
            'execution_time': execution_time,
            'result_size': len(str(result)),
            'thread_id': threading.get_ident()
        }
        
        self.analysis_logger.info(f"ë¶„ì„ ì™„ë£Œ: {json.dumps(log_data, ensure_ascii=False)}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
        self.performance_metrics[f"{ticker}_{operation}"].append({
            'execution_time': execution_time,
            'timestamp': datetime.now().isoformat(),
            'result_size': len(str(result))
        })
        
        # ì„±ëŠ¥ ë¡œê·¸
        self.performance_logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_analysis_error(self, ticker: str, error: Exception, operation: str = 'analysis'):
        """ë¶„ì„ ì—ëŸ¬ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'ticker': ticker,
            'status': 'error',
            'error_type': type(error).__name__,
            'error_message': str(error),
            'thread_id': threading.get_ident()
        }
        
        self.analysis_logger.error(f"ë¶„ì„ ì—ëŸ¬: {json.dumps(log_data, ensure_ascii=False)}")
        self.error_logger.error(f"ë¶„ì„ ì—ëŸ¬: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_performance(self, operation: str, duration: float, details: Dict = None):
        """ì„±ëŠ¥ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'duration': duration,
            'details': details or {},
            'thread_id': threading.get_ident()
        }
        
        self.performance_logger.info(f"ì„±ëŠ¥ ë¡œê·¸: {json.dumps(log_data, ensure_ascii=False)}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
        self.performance_metrics[operation].append({
            'duration': duration,
            'timestamp': datetime.now().isoformat(),
            'details': details or {}
        })
    
    def log_error(self, error: Exception, context: Dict = None, level: str = 'ERROR'):
        """ì—ëŸ¬ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context or {},
            'thread_id': threading.get_ident()
        }
        
        log_message = f"ì—ëŸ¬ ë¡œê·¸: {json.dumps(log_data, ensure_ascii=False)}"
        
        if level.upper() in self.log_levels:
            log_level = self.log_levels[level.upper()]
            self.error_logger.log(log_level, log_message)
        else:
            self.error_logger.error(log_message)
    
    def log_system_event(self, event: str, details: Dict = None):
        """ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'event': event,
            'details': details or {},
            'thread_id': threading.get_ident()
        }
        
        self.system_logger.info(f"ì‹œìŠ¤í…œ ì´ë²¤íŠ¸: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_cache_operation(self, operation: str, key: str, success: bool, details: Dict = None):
        """ìºì‹œ ì‘ì—… ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'key': key,
            'success': success,
            'details': details or {},
            'thread_id': threading.get_ident()
        }
        
        self.system_logger.info(f"ìºì‹œ ì‘ì—…: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_data_operation(self, operation: str, ticker: str, market_type: str, 
                          success: bool, details: Dict = None):
        """ë°ì´í„° ì‘ì—… ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'ticker': ticker,
            'market_type': market_type,
            'success': success,
            'details': details or {},
            'thread_id': threading.get_ident()
        }
        
        self.analysis_logger.info(f"ë°ì´í„° ì‘ì—…: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_batch_operation(self, operation: str, batch_size: int, success_count: int, 
                           error_count: int, details: Dict = None):
        """ë°°ì¹˜ ì‘ì—… ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'operation': operation,
            'batch_size': batch_size,
            'success_count': success_count,
            'error_count': error_count,
            'success_rate': (success_count / batch_size * 100) if batch_size > 0 else 0,
            'details': details or {},
            'thread_id': threading.get_ident()
        }
        
        self.system_logger.info(f"ë°°ì¹˜ ì‘ì—…: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_memory_usage(self, memory_info: Dict):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'memory_usage': memory_info,
            'thread_id': threading.get_ident()
        }
        
        self.performance_logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_api_call(self, endpoint: str, method: str, status_code: int, 
                     response_time: float, details: Dict = None):
        """API í˜¸ì¶œ ë¡œê¹…"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'endpoint': endpoint,
            'method': method,
            'status_code': status_code,
            'response_time': response_time,
            'details': details or {},
            'thread_id': threading.get_ident()
        }
        
        self.system_logger.info(f"API í˜¸ì¶œ: {json.dumps(log_data, ensure_ascii=False)}")
    
    def get_performance_statistics(self, operation: str = None, 
                                 time_range: timedelta = None) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if operation:
            metrics = self.performance_metrics.get(operation, [])
        else:
            metrics = []
            for op_metrics in self.performance_metrics.values():
                metrics.extend(op_metrics)
        
        if time_range:
            cutoff_time = datetime.now() - time_range
            metrics = [
                m for m in metrics 
                if datetime.fromisoformat(m['timestamp']) > cutoff_time
            ]
        
        if not metrics:
            return {
                'total_operations': 0,
                'average_duration': 0,
                'min_duration': 0,
                'max_duration': 0,
                'total_duration': 0
            }
        
        durations = [m.get('duration', 0) for m in metrics]
        
        return {
            'total_operations': len(metrics),
            'average_duration': sum(durations) / len(durations),
            'min_duration': min(durations),
            'max_duration': max(durations),
            'total_duration': sum(durations)
        }
    
    def get_error_statistics(self, time_range: timedelta = None) -> Dict:
        """ì—ëŸ¬ í†µê³„ ë°˜í™˜"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í†µê³„ ìƒì„±
        return {
            'total_errors': 0,
            'error_types': {},
            'recent_errors': []
        }
    
    def get_system_health_report(self) -> Dict:
        """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ë¦¬í¬íŠ¸"""
        return {
            'timestamp': datetime.now().isoformat(),
            'performance_stats': self.get_performance_statistics(),
            'error_stats': self.get_error_statistics(),
            'memory_usage': self._get_memory_usage(),
            'log_file_sizes': self._get_log_file_sizes()
        }
    
    def cleanup_old_logs(self, days_to_keep: int = 30):
        """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        log_dirs = ['logs', 'logs/analysis', 'logs/performance', 'logs/errors']
        
        for log_dir in log_dirs:
            if os.path.exists(log_dir):
                for file in os.listdir(log_dir):
                    file_path = os.path.join(log_dir, file)
                    if os.path.isfile(file_path):
                        file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                        if file_time < cutoff_date:
                            try:
                                os.remove(file_path)
                                self.system_logger.info(f"ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ: {file_path}")
                            except Exception as e:
                                self.error_logger.error(f"ë¡œê·¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path}, {str(e)}")
    
    def export_logs(self, export_path: str, log_types: List[str] = None):
        """ë¡œê·¸ ë‚´ë³´ë‚´ê¸°"""
        if log_types is None:
            log_types = ['analysis', 'performance', 'errors', 'system']
        
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'log_types': log_types,
            'logs': {}
        }
        
        for log_type in log_types:
            log_file = f'logs/{log_type}.log'
            if os.path.exists(log_file):
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        export_data['logs'][log_type] = f.readlines()
                except Exception as e:
                    self.error_logger.error(f"ë¡œê·¸ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {log_type}, {str(e)}")
        
        try:
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            self.system_logger.info(f"ë¡œê·¸ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_path}")
        except Exception as e:
            self.error_logger.error(f"ë¡œê·¸ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {export_path}, {str(e)}")
    
    def _get_memory_usage(self) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            return {
                'total_mb': memory.total / (1024 * 1024),
                'available_mb': memory.available / (1024 * 1024),
                'used_mb': memory.used / (1024 * 1024),
                'percent': memory.percent
            }
        except ImportError:
            return {'error': 'psutil not available'}
    
    def _get_log_file_sizes(self) -> Dict:
        """ë¡œê·¸ íŒŒì¼ í¬ê¸° ë°˜í™˜"""
        log_files = {
            'analysis': 'logs/analysis/analysis.log',
            'performance': 'logs/performance/performance.log',
            'errors': 'logs/errors/errors.log',
            'system': 'logs/system.log',
            'app': 'logs/app.log'
        }
        
        sizes = {}
        for name, file_path in log_files.items():
            if os.path.exists(file_path):
                try:
                    size = os.path.getsize(file_path)
                    sizes[name] = size
                except Exception:
                    sizes[name] = 0
            else:
                sizes[name] = 0
        
        return sizes

def logging_decorator(operation: str = 'unknown'):
    """ë¡œê¹… ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            logging_service = LoggingService()
            
            # í•¨ìˆ˜ ì •ë³´ ì¶”ì¶œ
            func_name = func.__name__
            ticker = kwargs.get('ticker', args[0] if args else 'unknown')
            market_type = kwargs.get('market_type', 'unknown')
            
            # ì‹œì‘ ë¡œê¹…
            logging_service.log_analysis_start(ticker, market_type, operation)
            
            try:
                # í•¨ìˆ˜ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ì™„ë£Œ ë¡œê¹…
                logging_service.log_analysis_complete(ticker, result, operation)
                
                return result
                
            except Exception as e:
                # ì—ëŸ¬ ë¡œê¹…
                logging_service.log_analysis_error(ticker, e, operation)
                raise
            
        return wrapper
    return decorator

def main():
    """ë©”ì¸ ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ Phase 4 ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # ë¡œê¹… ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    logging_service = LoggingService()
    
    # í…ŒìŠ¤íŠ¸ ë¡œê¹…
    print("\nğŸ“‹ ë¡œê¹… í…ŒìŠ¤íŠ¸:")
    
    # 1. ë¶„ì„ ì‹œì‘/ì™„ë£Œ ë¡œê¹…
    print("  1. ë¶„ì„ ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_analysis_start('005930.KS', 'KOSPI', 'stock_analysis')
    time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜ëœ ì²˜ë¦¬ ì‹œê°„
    logging_service.log_analysis_complete('005930.KS', {'status': 'success'}, 'stock_analysis')
    
    # 2. ì„±ëŠ¥ ë¡œê¹…
    print("  2. ì„±ëŠ¥ ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_performance('data_loading', 1.5, {'rows': 1000})
    logging_service.log_performance('analysis', 2.3, {'indicators': 5})
    
    # 3. ì—ëŸ¬ ë¡œê¹…
    print("  3. ì—ëŸ¬ ë¡œê¹… í…ŒìŠ¤íŠ¸")
    try:
        raise ValueError("í…ŒìŠ¤íŠ¸ ì—ëŸ¬")
    except Exception as e:
        logging_service.log_error(e, {'context': 'test'})
    
    # 4. ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë¡œê¹…
    print("  4. ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_system_event('cache_cleanup', {'files_removed': 5})
    logging_service.log_system_event('batch_processing', {'batch_size': 100})
    
    # 5. ìºì‹œ ì‘ì—… ë¡œê¹…
    print("  5. ìºì‹œ ì‘ì—… ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_cache_operation('get', 'test_key', True, {'hit': True})
    logging_service.log_cache_operation('set', 'test_key', True, {'expire': 3600})
    
    # 6. ë°ì´í„° ì‘ì—… ë¡œê¹…
    print("  6. ë°ì´í„° ì‘ì—… ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_data_operation('load', '005930.KS', 'KOSPI', True, {'rows': 1000})
    logging_service.log_data_operation('save', '005930.KS', 'KOSPI', True, {'format': 'csv'})
    
    # 7. ë°°ì¹˜ ì‘ì—… ë¡œê¹…
    print("  7. ë°°ì¹˜ ì‘ì—… ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_batch_operation('analysis', 10, 8, 2, {'market_type': 'KOSPI'})
    
    # 8. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
    print("  8. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… í…ŒìŠ¤íŠ¸")
    memory_info = logging_service._get_memory_usage()
    logging_service.log_memory_usage(memory_info)
    
    # 9. API í˜¸ì¶œ ë¡œê¹…
    print("  9. API í˜¸ì¶œ ë¡œê¹… í…ŒìŠ¤íŠ¸")
    logging_service.log_api_call('/api/stock/005930.KS', 'GET', 200, 0.5, {'cache': True})
    
    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ë¡œê¹… í†µê³„:")
    performance_stats = logging_service.get_performance_statistics()
    print(f"  ì„±ëŠ¥ í†µê³„: {performance_stats}")
    
    system_health = logging_service.get_system_health_report()
    print(f"  ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ: {system_health}")
    
    print("\n" + "=" * 60)
    print("âœ… Phase 4 ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    main() 