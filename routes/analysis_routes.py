import os
import logging
import pandas as pd
import base64
import io
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import mplfinance as mpf
import glob

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Malgun Gothic', 'NanumGothic', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from flask import Blueprint, render_template, request, jsonify, current_app, redirect, url_for
from flask_login import login_required
import google.generativeai as genai
import requests
import json
import ta

# Services
from services.market.data_download_service import DataDownloadService
from services.technical_indicators_service import technical_indicators_service
from services.analysis.ai_analysis_service import AIAnalysisService
from services.analysis.chart_service import ChartService
from services.market.data_reading_service import DataReadingService
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from services.core.unified_market_analysis_service import UnifiedMarketAnalysisService
from services.core.cache_service import CacheService
from services.technical_indicators_service import TechnicalIndicatorsService
from services.analysis.crossover.detection import CrossoverDetector
from models import Stock

# Config
from config import DevelopmentConfig

# Blueprint ìƒì„±
analysis_bp = Blueprint('analysis', __name__, url_prefix='/analysis')

def _get_market_timezone(market: str) -> ZoneInfo:
    try:
        m = (market or '').upper()
        if m in ['KOSPI', 'KOSDAQ']:
            return ZoneInfo('Asia/Seoul')
        return ZoneInfo('America/New_York')
    except Exception:
        return ZoneInfo('Asia/Seoul')

def _get_market_hours(market: str) -> tuple:
    """Return (start_h, start_m, close_h, close_m) in local market time."""
    m = (market or '').upper()
    if m in ['KOSPI', 'KOSDAQ']:
        return (9, 0, 15, 30)  # 09:00-15:30 KST
    return (9, 30, 16, 0)     # 09:30-16:00 EST

def _prev_business_day(dt_local: datetime) -> datetime:
    d = dt_local.date()
    while True:
        d = d - timedelta(days=1)
        if d.weekday() < 5:
            return datetime(d.year, d.month, d.day, tzinfo=dt_local.tzinfo)

def _find_latest_cached_analysis_file(ticker: str, market: str) -> str | None:
    try:
        analysis_dir = os.path.join("static", "analysis")
        pattern = os.path.join(analysis_dir, f"{ticker}_AI_Analysis_{market}_*.html")
        files = glob.glob(pattern)
        if not files:
            return None
        files.sort(key=lambda p: os.path.getmtime(p))
        return files[-1]
    except Exception:
        return None

def _detect_canonical_market_by_files(ticker: str, preferred_market: str) -> str:
    try:
        candidates = [preferred_market] + [m for m in ['US', 'KOSPI', 'KOSDAQ'] if m != preferred_market]
        for cand in candidates:
            try:
                data_dir = os.path.join("static", "data", cand)
                if glob.glob(os.path.join(data_dir, f"{ticker}_indicators_d_*.csv")) or \
                   glob.glob(os.path.join(data_dir, f"{ticker}_ohlcv_d_*.csv")):
                    return cand
            except Exception:
                continue
        return preferred_market
    except Exception:
        return preferred_market

def _get_file_created_dt_in_tz(path: str, tz: ZoneInfo) -> datetime | None:
    try:
        mtime = os.path.getmtime(path)
        return datetime.fromtimestamp(mtime, tz)
    except Exception:
        return None

def _determine_market_phase(now_local: datetime, market: str) -> str:
    sh, sm, ch, cm = _get_market_hours(market)
    start_dt = now_local.replace(hour=sh, minute=sm, second=0, microsecond=0)
    close_dt = now_local.replace(hour=ch, minute=cm, second=0, microsecond=0)
    if now_local < start_dt:
        return 'pre'
    if now_local <= close_dt:
        return 'open'
    return 'post'

def _get_prev_close_and_today_close(now_local: datetime, market: str) -> tuple[datetime, datetime]:
    sh, sm, ch, cm = _get_market_hours(market)
    today_close = now_local.replace(hour=ch, minute=cm, second=0, microsecond=0)
    prev_biz = _prev_business_day(now_local)
    prev_close = prev_biz.replace(hour=ch, minute=cm, second=0, microsecond=0)
    return prev_close, today_close

def _try_return_cached_analysis_html(ticker: str, market: str) -> str | None:
    try:
        tz = _get_market_timezone(market)
        now_local = datetime.now(tz)
        latest_path = _find_latest_cached_analysis_file(ticker, market)
        prev_close_dt, today_close_dt = _get_prev_close_and_today_close(now_local, market)
        phase = _determine_market_phase(now_local, market)

        logging.info(f"[CACHE] {ticker}/{market} phase={phase} now={now_local} prev_close={prev_close_dt} today_close={today_close_dt} latest_path={latest_path}")

        if not latest_path:
            logging.info(f"[CACHE] No cached file found for {ticker}/{market}")
            return None

        created_dt = _get_file_created_dt_in_tz(latest_path, tz)
        if created_dt is None:
            return None

        if phase in ('pre', 'open'):
            is_fresh = created_dt >= prev_close_dt
        else:  # post
            is_fresh = created_dt > today_close_dt

        logging.info(f"[CACHE] created={created_dt} fresh={is_fresh}")

        if not is_fresh:
            logging.info(f"[CACHE] Not fresh: created={created_dt} phase={phase} prev_close={prev_close_dt} today_close={today_close_dt}")
            return None

        try:
            with open(latest_path, 'r', encoding='utf-8') as f:
                html = f.read()
            logging.info(f"[CACHE] Using cached analysis HTML: {latest_path}")
            return html
        except Exception as read_err:
            logging.warning(f"[CACHE] Failed to read cached file: {read_err}")
            return None
    except Exception as e:
        logging.warning(f"[CACHE] Unexpected error: {e}")
        return None

def generate_charts(ticker, market_type='KOSPI'):
    """ì¼ë´‰, ì£¼ë´‰, ì›”ë´‰ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ìƒˆë¡œìš´ ì°¨íŠ¸ ì„œë¹„ìŠ¤ ì‚¬ìš©
        from services.analysis.chart_service import ChartService
        chart_service = ChartService()
        
        # ì°¨íŠ¸ ìƒì„±
        charts, error = chart_service.generate_charts(ticker, market_type)
        
        return charts, error
        
    except Exception as e:
        error_msg = f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg)
        return None, error_msg
        
        # ì´ í•¨ìˆ˜ëŠ” ì´ì œ ChartServiceë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
        # ê¸°ì¡´ ì½”ë“œëŠ” ì œê±°í•˜ê³  ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

def create_chart_image(df, title, timeframe, ticker, market_type='US'):
    """ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  base64ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    try:
        # âœ… ìƒˆë¡œìš´ ì°¨íŠ¸ ì„œë¹„ìŠ¤ ì‚¬ìš© (ì¤‘ë³µ ê³„ì‚° ì œê±°ë¨)
        from services.analysis.chart_service import ChartService
        chart_service = ChartService()
        
        # ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        chart_image = chart_service.create_chart_image(df, title, timeframe, ticker, market_type)
        
        return chart_image
        
    except Exception as e:
        error_msg = f"ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg)
        return None

# ğŸš« ì˜¤ë˜ëœ ì°¨íŠ¸ ìƒì„± ì½”ë“œ ì™„ì „ ì œê±°ë¨ - ChartServiceë¥¼ ì‚¬ìš©í•˜ì„¸ìš”

def load_saved_indicators(df, ticker, timeframe, market_type='US'):
    """
    indicators CSVì—ì„œ ì§€í‘œ ë¡œë“œ
    ê³„ì‚° ë¡œì§ ì œê±°, ìˆœìˆ˜ ì½ê¸°ë§Œ
    """
    try:
        # ì‹œê°„ëŒ€ ë§¤í•‘
        timeframe_map = {
            'ì¼ë´‰': 'd',
            'ì£¼ë´‰': 'w', 
            'ì›”ë´‰': 'm'
        }
        tf = timeframe_map.get(timeframe, 'd')
        
        # âœ… ìƒˆë¡œìš´: CSV ì½ê¸°ë§Œ
        indicators_df = get_indicators_data(ticker, tf, market_type)
        
        if indicators_df.empty:
            logging.error(f"ì§€í‘œ ë°ì´í„° ì—†ìŒ: {ticker}")
            return df
        
        # OHLCV ë°ì´í„°ì™€ ì§€í‘œ ë°ì´í„° ê²°í•©
        combined_df = df.join(indicators_df, how='left')
        return combined_df
        
    except Exception as e:
        logging.error(f"ì§€í‘œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return df

def get_indicators_data(ticker, timeframe, market_type='US'):
    """indicators CSVì—ì„œ ì§€í‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        from services.technical_indicators_service import TechnicalIndicatorsService
        indicators_service = TechnicalIndicatorsService()
        
        # ì§€í‘œ CSV í™•ì¸ ë° ìë™ ìƒì„±
        if not indicators_service.ensure_indicators_exist(ticker, timeframe, market_type):
            logging.error(f"[{ticker}] ì§€í‘œ CSV ìƒì„± ì‹¤íŒ¨")
            return pd.DataFrame()
        
        return indicators_service.read_indicators_csv(ticker, market_type, timeframe)
    except Exception as e:
        logging.error(f"ì§€í‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# ğŸš« ì¤‘ë³µ ê³„ì‚° í•¨ìˆ˜ ì œê±°ë¨ - TechnicalIndicatorsServiceë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
# def calculate_technical_indicators(df): # ì´ í•¨ìˆ˜ëŠ” ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤

def get_ohlcv_data_for_ai(ticker, market_type='KOSPI'):
    """AI ë¶„ì„ìš© OHLCV ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        ticker = ticker.upper()
        
        # ë””ë²„ê·¸: ë°ì´í„° ë¡œë”© ê³¼ì • ë¡œê·¸ ì €ì¥
        debug_log_path = os.path.join("logs", f"data_loading_debug_{ticker}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        os.makedirs("logs", exist_ok=True)
        
        with open(debug_log_path, 'w', encoding='utf-8') as f:
            f.write(f"=== AI ë¶„ì„ìš© ë°ì´í„° ë¡œë”© ë””ë²„ê·¸ ë¡œê·¸ ===\n")
            f.write(f"í‹°ì»¤: {ticker}\n")
            f.write(f"ì‹œì¥íƒ€ì…: {market_type}\n")
            f.write(f"ë¡œë”© ì‹œê°„: {datetime.now()}\n")
        
        # market_typeì„ ì‹¤ì œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜
        if market_type.upper() in ['KOSPI', 'KOSDAQ']:
            actual_market_type = market_type.upper()
        elif market_type.upper() == 'US':
            actual_market_type = 'US'
        else:
            # ê¸°ë³¸ê°’ì€ KOSPI
            actual_market_type = 'KOSPI'
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ì‹¤ì œ ì‹œì¥íƒ€ì…: {actual_market_type}\n")
        
        # ì €ì¥ëœ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        data_dir = os.path.join("static/data", actual_market_type)
        if not os.path.exists(data_dir):
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {data_dir}\n")
            return None, None, None
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}\n")
        
        # ìµœì‹  íŒŒì¼ ì°¾ê¸°
        ohlcv_files = []
        for filename in os.listdir(data_dir):
            if filename.startswith(f"{ticker}_") and filename.endswith("_ohlcv_d.csv"):
                ohlcv_files.append(filename)
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ë°œê²¬ëœ ì¼ë´‰ íŒŒì¼ë“¤: {ohlcv_files}\n")
        
        if not ohlcv_files:
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write("ì¼ë´‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\n")
            return None, None, None
        
        # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
        latest_file = max(ohlcv_files)
        ohlcv_path = os.path.join(data_dir, latest_file)
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ì„ íƒëœ ì¼ë´‰ íŒŒì¼: {latest_file}\n")
            f.write(f"ì¼ë´‰ íŒŒì¼ ê²½ë¡œ: {ohlcv_path}\n")
        
        # OHLCV ë°ì´í„° ë¡œë“œ
        # [ë©”ëª¨] 2025-08-19: CSV ì§ì ‘ íŒŒì‹±ì„ ì¤‘ë‹¨í•˜ê³  DataReadingServiceë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # ê¸°ì¡´ ì½”ë“œ: df = pd.read_csv(ohlcv_path, index_col=0)
        try:
            from services.market.data_reading_service import DataReadingService
            _drs_tmp = DataReadingService()
            # market_typeì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í´ë”ëª… ì¶”ì¶œ ì‹œë„, ì‹¤íŒ¨ ì‹œ 'US'
            import os
            market_guess = 'US'
            try:
                parts = os.path.normpath(ohlcv_path).split(os.sep)
                if len(parts) >= 2:
                    market_guess = parts[-2].upper()
            except Exception:
                pass
            df = _drs_tmp.read_ohlcv_csv(ticker, market_guess, 'd')
        except Exception as _:
            df = pd.DataFrame()
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ë¡œë“œëœ ì¼ë´‰ ë°ì´í„° í–‰ ìˆ˜: {len(df)}\n")
            f.write(f"ì¼ë´‰ ë°ì´í„° ì»¬ëŸ¼: {df.columns.tolist()}\n")
            f.write(f"ì¼ë´‰ ë°ì´í„° ì¸ë±ìŠ¤ (ì²˜ìŒ 5ê°œ): {df.index[:5].tolist()}\n")
            f.write(f"ì¼ë´‰ ë°ì´í„° ì¸ë±ìŠ¤ (ë§ˆì§€ë§‰ 5ê°œ): {df.index[-5:].tolist()}\n")
        
        if df.empty:
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write("ì¼ë´‰ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ\n")
            return None, None, None
        
        # ì¸ë±ìŠ¤ë¥¼ DatetimeIndexë¡œ ë³€í™˜
        try:
            df.index = pd.to_datetime(df.index, utc=True)
            # UTC ì‹œê°„ëŒ€ ì œê±° (ë¡œì»¬ ì‹œê°„ìœ¼ë¡œ ë³€í™˜)
            df.index = df.index.tz_localize(None)
            
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write(f"ì‹œê°„ëŒ€ ë³€í™˜ í›„ ì¸ë±ìŠ¤ (ì²˜ìŒ 5ê°œ): {df.index[:5].tolist()}\n")
                f.write(f"ì‹œê°„ëŒ€ ë³€í™˜ í›„ ì¸ë±ìŠ¤ (ë§ˆì§€ë§‰ 5ê°œ): {df.index[-5:].tolist()}\n")
        except Exception as e:
            logging.error(f"Failed to convert index to datetime: {e}")
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write(f"ì¸ë±ìŠ¤ ë³€í™˜ ì‹¤íŒ¨: {e}\n")
            return None, None, None
        
        # OHLCV ì»¬ëŸ¼ ì •ê·œí™”
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        required_columns = ["Open", "High", "Low", "Close", "Volume"]
        if not all(col in df.columns for col in required_columns):
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŒ. í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}\n")
            return None, None, None
        
        df = df[required_columns].dropna()
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ì •ê·œí™” í›„ ë°ì´í„° í–‰ ìˆ˜: {len(df)}\n")
            f.write(f"ì •ê·œí™” í›„ ì»¬ëŸ¼: {df.columns.tolist()}\n")
        
        # ì¼ë´‰ ë°ì´í„°
        daily_data = format_ohlcv_data(df, "ì¼ë´‰")
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"í¬ë§·íŒ…ëœ ì¼ë´‰ ë°ì´í„° ê°œìˆ˜: {len(daily_data)}\n")
            if daily_data:
                f.write(f"ì¼ë´‰ ë°ì´í„° ìƒ˜í”Œ (ìµœê·¼ 5ê°œ):\n")
                for i, item in enumerate(daily_data[-5:]):
                    f.write(f"  {i+1}. {item}\n")
        
        # ì£¼ë´‰ ë°ì´í„° - ì €ì¥ëœ ì£¼ë´‰ ë°ì´í„° ì‚¬ìš©
        weekly_files = []
        for filename in os.listdir(data_dir):
            if filename.startswith(f"{ticker}_") and filename.endswith("_ohlcv_w.csv"):
                weekly_files.append(filename)
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ë°œê²¬ëœ ì£¼ë´‰ íŒŒì¼ë“¤: {weekly_files}\n")
        
        if weekly_files:
            latest_weekly_file = max(weekly_files)
            weekly_path = os.path.join(data_dir, latest_weekly_file)
            # [ë©”ëª¨] 2025-08-19: ì£¼ë´‰ë„ ì„œë¹„ìŠ¤ í˜¸ì¶œë¡œ ëŒ€ì²´
            # ê¸°ì¡´ ì½”ë“œ: weekly_df = pd.read_csv(weekly_path, index_col=0)
            try:
                weekly_df = _drs_tmp.read_ohlcv_csv(ticker, market_guess, 'w')
            except Exception:
                weekly_df = pd.DataFrame()
            
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write(f"ì„ íƒëœ ì£¼ë´‰ íŒŒì¼: {latest_weekly_file}\n")
                f.write(f"ë¡œë“œëœ ì£¼ë´‰ ë°ì´í„° í–‰ ìˆ˜: {len(weekly_df)}\n")
            
            try:
                weekly_df.index = pd.to_datetime(weekly_df.index, utc=True)
                weekly_df.index = weekly_df.index.tz_localize(None)
            except Exception as e:
                logging.error(f"Failed to convert weekly index to datetime: {e}")
                weekly_df = pd.DataFrame()
            
            if isinstance(weekly_df.columns, pd.MultiIndex):
                weekly_df.columns = weekly_df.columns.get_level_values(0)
            
            if not weekly_df.empty and all(col in weekly_df.columns for col in required_columns):
                weekly_df = weekly_df[required_columns].dropna()
                weekly_data = format_ohlcv_data(weekly_df, "ì£¼ë´‰")
                
                with open(debug_log_path, 'a', encoding='utf-8') as f:
                    f.write(f"í¬ë§·íŒ…ëœ ì£¼ë´‰ ë°ì´í„° ê°œìˆ˜: {len(weekly_data)}\n")
                    if weekly_data:
                        f.write(f"ì£¼ë´‰ ë°ì´í„° ìƒ˜í”Œ (ìµœê·¼ 5ê°œ):\n")
                        for i, item in enumerate(weekly_data[-5:]):
                            f.write(f"  {i+1}. {item}\n")
            else:
                weekly_data = None
                with open(debug_log_path, 'a', encoding='utf-8') as f:
                    f.write("ì£¼ë´‰ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨\n")
        else:
            weekly_data = None
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write("ì£¼ë´‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\n")
        
        # ì›”ë´‰ ë°ì´í„° - ì €ì¥ëœ ì›”ë´‰ ë°ì´í„° ì‚¬ìš©
        monthly_files = []
        for filename in os.listdir(data_dir):
            if filename.startswith(f"{ticker}_") and filename.endswith("_ohlcv_m.csv"):
                monthly_files.append(filename)
        
        with open(debug_log_path, 'a', encoding='utf-8') as f:
            f.write(f"ë°œê²¬ëœ ì›”ë´‰ íŒŒì¼ë“¤: {monthly_files}\n")
        
        if monthly_files:
            latest_monthly_file = max(monthly_files)
            monthly_path = os.path.join(data_dir, latest_monthly_file)
            # [ë©”ëª¨] 2025-08-19: ì›”ë´‰ë„ ì„œë¹„ìŠ¤ í˜¸ì¶œë¡œ ëŒ€ì²´
            # ê¸°ì¡´ ì½”ë“œ: monthly_df = pd.read_csv(monthly_path, index_col=0)
            try:
                monthly_df = _drs_tmp.read_ohlcv_csv(ticker, market_guess, 'm')
            except Exception:
                monthly_df = pd.DataFrame()
            
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write(f"ì„ íƒëœ ì›”ë´‰ íŒŒì¼: {latest_monthly_file}\n")
                f.write(f"ë¡œë“œëœ ì›”ë´‰ ë°ì´í„° í–‰ ìˆ˜: {len(monthly_df)}\n")
            
            try:
                monthly_df.index = pd.to_datetime(monthly_df.index, utc=True)
                monthly_df.index = monthly_df.index.tz_localize(None)
            except Exception as e:
                logging.error(f"Failed to convert monthly index to datetime: {e}")
                monthly_df = pd.DataFrame()
            
            if isinstance(monthly_df.columns, pd.MultiIndex):
                monthly_df.columns = monthly_df.columns.get_level_values(0)
            
            if not monthly_df.empty and all(col in monthly_df.columns for col in required_columns):
                monthly_df = monthly_df[required_columns].dropna()
                monthly_data = format_ohlcv_data(monthly_df, "ì›”ë´‰")
                
                with open(debug_log_path, 'a', encoding='utf-8') as f:
                    f.write(f"í¬ë§·íŒ…ëœ ì›”ë´‰ ë°ì´í„° ê°œìˆ˜: {len(monthly_data)}\n")
                    if monthly_data:
                        f.write(f"ì›”ë´‰ ë°ì´í„° ìƒ˜í”Œ (ìµœê·¼ 5ê°œ):\n")
                        for i, item in enumerate(monthly_data[-5:]):
                            f.write(f"  {i+1}. {item}\n")
            else:
                monthly_data = None
                with open(debug_log_path, 'a', encoding='utf-8') as f:
                    f.write("ì›”ë´‰ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨\n")
        else:
            monthly_data = None
            with open(debug_log_path, 'a', encoding='utf-8') as f:
                f.write("ì›”ë´‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\n")
        
        logging.info(f"ë°ì´í„° ë¡œë”© ë””ë²„ê·¸ ë¡œê·¸ ì €ì¥ë¨: {debug_log_path}")
        
        return daily_data, weekly_data, monthly_data
        
    except Exception as e:
        logging.error(f"Error in get_ohlcv_data_for_ai for {ticker}: {e}")
        return None, None, None

def format_ohlcv_data(df, timeframe):
    """OHLCV ë°ì´í„°ë¥¼ AI ë¶„ì„ìš©ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    try:
        formatted_data = []
        for date, row in df.iterrows():
            formatted_data.append({
                'date': date.strftime('%Y-%m-%d'),
                'open': round(row['Open'], 2),
                'high': round(row['High'], 2),
                'low': round(row['Low'], 2),
                'close': round(row['Close'], 2),
                'volume': int(row['Volume'])
            })
        
        return formatted_data
        
    except Exception as e:
        logging.error(f"Error formatting OHLCV data: {e}")
        return []

def perform_ai_analysis(ticker, daily_data, weekly_data, monthly_data, market_type='KOSPI', company_name=None):
    """AI ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        # ìƒˆë¡œìš´ AI ë¶„ì„ ì„œë¹„ìŠ¤ ì‚¬ìš©
        from services.analysis.ai_analysis_service import AIAnalysisService
        ai_service = AIAnalysisService()
        
        # AI ë¶„ì„ ìˆ˜í–‰
        analysis_result, error = ai_service.analyze_stock(ticker, market_type, company_name)
        
        return analysis_result, error
        
    except Exception as e:
        error_msg = f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg)
        return None, error_msg

def perform_ai_analysis_with_data(ticker, daily_data, weekly_data, monthly_data, market_type='KOSPI', company_name=None):
    """[2025-08-09 ì¶”ê°€] ì§€í‘œ í¬í•¨ ë°ì´í„°ë¡œ AI ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        from services.analysis.ai_analysis_service import AIAnalysisService
        ai_service = AIAnalysisService()
        
        # ì´ë¯¸ ë¡œë”©ëœ ì§€í‘œ í¬í•¨ ë°ì´í„°ë¥¼ AI ì„œë¹„ìŠ¤ì— ì§ì ‘ ì „ë‹¬
        analysis_result, error = ai_service.analyze_stock_with_data(
            ticker, daily_data, weekly_data, monthly_data, market_type, company_name
        )
        
        logging.info(f"[{ticker}] AI ë¶„ì„ ì™„ë£Œ - ì§€í‘œ í¬í•¨ ë°ì´í„° ì‚¬ìš©")
        return analysis_result, error
        
    except Exception as e:
        error_msg = f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg)
        return None, error_msg

def get_ohlcv_with_indicators(ticker, timeframe, market_type='KOSPI'):
    """OHLCV ë°ì´í„°ì™€ ì§€í‘œ ë°ì´í„°ë¥¼ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        import glob
        import pandas as pd
        
        logging.info(f"get_ohlcv_with_indicators: ì‹œì‘ - í‹°ì»¤: {ticker}, íƒ€ì„í”„ë ˆì„: {timeframe}, ì‹œì¥: {market_type}")
        
        # market_typeì„ ì‹¤ì œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜
        if market_type.upper() in ['KOSPI', 'KOSDAQ']:
            actual_market_type = market_type.upper()
        elif market_type.upper() == 'US':
            actual_market_type = 'US'
        else:
            # ê¸°ë³¸ê°’ì€ KOSPI
            actual_market_type = 'KOSPI'
        
        logging.info(f"get_ohlcv_with_indicators: ì‹¤ì œ í´ë”ëª…: {actual_market_type}")
        # ë””ë²„ê·¸: ìµœì‹  íŒŒì¼ ê²½ë¡œ í™•ì¸ (OHLCV / Indicators)
        try:
            from services.market.file_management_service import FileManagementService
            fms = FileManagementService()
            latest_ohlcv_path = fms.get_latest_file(ticker, 'ohlcv', actual_market_type, timeframe)
            latest_ind_path = fms.get_latest_file(ticker, 'indicators', actual_market_type, timeframe)
            logging.info(f"get_ohlcv_with_indicators: ìµœì‹  ê²½ë¡œ í™•ì¸ - OHLCV={latest_ohlcv_path}, IND={latest_ind_path}")
        except Exception as path_err:
            logging.warning(f"get_ohlcv_with_indicators: ìµœì‹  ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {path_err}")
        
        # [2025-08-09 ìˆ˜ì • ì „ ì½”ë“œ - ì£¼ì„ ì²˜ë¦¬]
        # ì§€í‘œ íŒŒì¼ íŒ¨í„´ (ìì²´ ê²€ìƒ‰ ë°©ì‹ - ë¬¸ì œ ìˆìŒ)
        # data_dir = os.path.join("static/data", actual_market_type)
        # indicator_pattern = os.path.join(data_dir, f"{ticker}_indicators_{timeframe}_*.csv")
        # logging.info(f"get_ohlcv_with_indicators: ì§€í‘œ íŒŒì¼ íŒ¨í„´: {indicator_pattern}")
        # indicator_files = glob.glob(indicator_pattern)
        
        # [2025-08-09 ìˆ˜ì •] admin_homeê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì§€í‘œ ë°ì´í„° ë¡œë“œ
        try:
            from services.technical_indicators_service import TechnicalIndicatorsService
            technical_service = TechnicalIndicatorsService()
            
            # ì§€í‘œ ë°ì´í„° ë¡œë“œ (ì¸ì ìˆœì„œ êµì •, caller ì œê±°)
            indicators_df = technical_service.read_indicators_csv(ticker, actual_market_type, timeframe)
            
            # None ì²´í¬ ë° DataFrame ê²€ì¦ ê°•í™”
            if indicators_df is None:
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] {timeframe} TechnicalIndicatorsServiceì—ì„œ None ë°˜í™˜")
                indicators_df = pd.DataFrame()  # ë¹ˆ DataFrameìœ¼ë¡œ ì´ˆê¸°í™”
            elif not isinstance(indicators_df, pd.DataFrame):
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] {timeframe} TechnicalIndicatorsServiceì—ì„œ DataFrameì´ ì•„ë‹Œ íƒ€ì… ë°˜í™˜: {type(indicators_df)}")
                indicators_df = pd.DataFrame()  # ë¹ˆ DataFrameìœ¼ë¡œ ì´ˆê¸°í™”
            
            if indicators_df.empty:
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] {timeframe}ì— ëŒ€í•œ ì§€í‘œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ (admin_home ë°©ì‹)")
                # ê¸°ë³¸ OHLCV ë°ì´í„° ë°˜í™˜
                daily_data, weekly_data, monthly_data = get_ohlcv_data_for_ai(ticker, market_type)
                
                if timeframe == 'd':
                    return daily_data
                elif timeframe == 'w':
                    return weekly_data
                elif timeframe == 'm':
                    return monthly_data
                else:
                    return daily_data
            
            logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë°ì´í„° ë¡œë”© ì„±ê³µ (admin_home ë°©ì‹) - í˜•íƒœ: {indicators_df.shape}")
            
        except Exception as e:
            logging.error(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë¡œë”© ì‹¤íŒ¨ (admin_home ë°©ì‹): {e}")
            # Fallback: ì„œë¹„ìŠ¤ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì§ì ‘ CSV íŒŒì‹± ì œê±°, ë©”íƒ€ë°ì´í„° ë¬¸ì œ ë°©ì§€)
            try:
                indicators_df = technical_service.read_indicators_csv(ticker, actual_market_type, timeframe)
                try:
                    shape_info = getattr(indicators_df, 'shape', None)
                    cols_preview = list(indicators_df.columns)[:12] if hasattr(indicators_df, 'columns') else []
                    logging.info(f"get_ohlcv_with_indicators: [{ticker}] indicators_df ë¡œë”© ì™„ë£Œ - shape={shape_info}, cols_preview={cols_preview}")
                except Exception as info_err:
                    logging.warning(f"get_ohlcv_with_indicators: indicators_df ì •ë³´ ë¡œê¹… ì‹¤íŒ¨: {info_err}")
            except Exception as e2:
                logging.error(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë¡œë”© ì¬ì‹œë„ ì‹¤íŒ¨: {e2}")
                indicators_df = pd.DataFrame()
        
        logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë°ì´í„° í˜•íƒœ: {indicators_df.shape}")
        logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë°ì´í„° ì»¬ëŸ¼ë“¤: {list(indicators_df.columns)}")
        
        if indicators_df.empty:
            logging.warning(f"get_ohlcv_with_indicators: [{ticker}] {timeframe}ì— ëŒ€í•œ ì§€í‘œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
            # get_ohlcv_data_for_aiëŠ” íŠœí”Œì„ ë°˜í™˜í•˜ë¯€ë¡œ timeframeì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„° ë°˜í™˜
            daily_data, weekly_data, monthly_data = get_ohlcv_data_for_ai(ticker, market_type)
            
            if timeframe == 'd':
                logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì¼ë´‰ ë°ì´í„° ë°˜í™˜ (ê°œìˆ˜: {len(daily_data) if daily_data else 0})")
                return daily_data
            elif timeframe == 'w':
                logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì£¼ë´‰ ë°ì´í„° ë°˜í™˜ (ê°œìˆ˜: {len(weekly_data) if weekly_data else 0})")
                return weekly_data
            elif timeframe == 'm':
                logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì›”ë´‰ ë°ì´í„° ë°˜í™˜ (ê°œìˆ˜: {len(monthly_data) if monthly_data else 0})")
                return monthly_data
            else:
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] ì•Œ ìˆ˜ ì—†ëŠ” timeframe: {timeframe}, ì¼ë´‰ ë°ì´í„° ë°˜í™˜")
                return daily_data
        
        # OHLCV + ì§€í‘œ + CrossInfo ë³‘í•©
        try:
            from services.market.data_reading_service import DataReadingService
            drs = DataReadingService()
            ohlcv_df = drs.read_ohlcv_csv(ticker, actual_market_type, timeframe)
            if ohlcv_df is None or ohlcv_df.empty:
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] OHLCV ë°ì´í„° ì—†ìŒ ({timeframe})")
            else:
                # 2025-08-16: ë³‘í•© ì¶©ëŒ ë°©ì§€ - indicators_dfì˜ OHLCV/ë©”íƒ€ ì»¬ëŸ¼ ì „ì²´ ì œê±° í›„ ë³‘í•©
                try:
                    drop_candidates = {'open','high','low','close','volume','adj close','date','date_index','time_index'}
                    cols_to_drop = [col for col in indicators_df.columns if str(col).strip().lower() in drop_candidates]
                    if cols_to_drop:
                        logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ DF ì¶©ëŒ ì»¬ëŸ¼ ì œê±°: {cols_to_drop}")
                        indicators_df = indicators_df.drop(columns=cols_to_drop, errors='ignore')
                except Exception as drop_err:
                    logging.warning(f"get_ohlcv_with_indicators: [{ticker}] ì¶©ëŒ ì»¬ëŸ¼ ì œê±° ì¤‘ ê²½ê³ : {drop_err}")

                # ì¸ë±ìŠ¤ ì‹œê°„ëŒ€ ì •ê·œí™”(ì–‘ìª½ tz-naive)
                try:
                    if isinstance(ohlcv_df.index, pd.DatetimeIndex) and ohlcv_df.index.tz is not None:
                        ohlcv_df.index = ohlcv_df.index.tz_localize(None)
                    if isinstance(indicators_df.index, pd.DatetimeIndex) and indicators_df.index.tz is not None:
                        indicators_df.index = indicators_df.index.tz_localize(None)
                except Exception:
                    pass

                indicators_df = ohlcv_df.join(indicators_df, how='left')
                logging.info(f"get_ohlcv_with_indicators: [{ticker}] OHLCV+ì§€í‘œ ë³‘í•© ì™„ë£Œ: {indicators_df.shape}")
            # CrossInfo ë³‘í•© (ê°­/EMA ë°°ì—´ ì •ë³´) - ì»¬ëŸ¼ ìë™ ë§¤í•‘ í¬í•¨
            try:
                cross_df = drs.read_crossinfo_csv(ticker, actual_market_type)
                if cross_df is not None and not cross_df.empty:
                    # ë‚ ì§œ ì»¬ëŸ¼ ì •ê·œí™”
                    if 'Date' in cross_df.columns:
                        cross_df['Date'] = pd.to_datetime(cross_df['Date'])
                        cross_df.set_index('Date', inplace=True)
                    # ìë™ ë§¤í•‘: ë‹¤ì–‘í•œ ì»¬ëŸ¼ëª… ë³€í˜•ì„ í‘œì¤€ í‚¤ë¡œ ì •ê·œí™”
                    import re
                    def _norm(s: str) -> str:
                        return re.sub(r"[^a-z0-9]", "", s.lower())

                    target_to_aliases = {
                        'Close_Gap_EMA20': ['Close_Gap_EMA20','CloseGapEMA20','close_gap_ema20','Close-EMA20-Gap','Gap_EMA20','GapEMA20','Close_EMA20_Gap'],
                        'Close_Gap_EMA40': ['Close_Gap_EMA40','CloseGapEMA40','close_gap_ema40','Close-EMA40-Gap','Gap_EMA40','GapEMA40','Close_EMA40_Gap'],
                        'EMA_Array_Order': ['EMA_Array_Order','EMAArrayOrder','ema_array_order','EMA_Order','EMA Array Order','EmaArrayOrder']
                    }
                    normalized_aliases = {t: {_norm(a) for a in aliases} for t, aliases in target_to_aliases.items()}
                    resolved_cols = {}  # {target: original_col}
                    for original_col in list(cross_df.columns):
                        ncol = _norm(original_col)
                        for target, nset in normalized_aliases.items():
                            if ncol in nset and target not in resolved_cols:
                                resolved_cols[target] = original_col
                                break
                    if resolved_cols:
                        sub_df = cross_df[list(resolved_cols.values())].rename(columns={v: k for k, v in resolved_cols.items()})
                        indicators_df = indicators_df.join(sub_df, how='left')
                        logging.info(f"get_ohlcv_with_indicators: [{ticker}] CrossInfo ìë™ë§¤í•‘ ë³‘í•© ì™„ë£Œ: targets={list(resolved_cols.keys())}")
            except Exception as cx_err:
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] CrossInfo ë³‘í•© ì‹¤íŒ¨: {cx_err}")
        except Exception as merge_err:
            logging.warning(f"get_ohlcv_with_indicators: [{ticker}] ë³‘í•© ì¤€ë¹„ ì‹¤íŒ¨: {merge_err}")

        # ë³‘í•© í›„ ìµœê·¼ 31ê°œ ë°ì´í„° ì„ íƒ (ì§€í‘œ ê³„ì‚°ì„ ìœ„í•´ 31ê°œ í•„ìš”)
        core_cols = ['Open','High','Low','Close','Volume']
        missing_core = [c for c in core_cols if c not in indicators_df.columns]
        if missing_core:
            logging.warning(f"get_ohlcv_with_indicators: [{ticker}] í•µì‹¬ OHLCV ì»¬ëŸ¼ ëˆ„ë½: {missing_core} (ë³‘í•© ì‹¤íŒ¨ ê°€ëŠ¥)")
        recent_data = indicators_df.tail(31)
        logging.info(f"get_ohlcv_with_indicators: [{ticker}] ìµœê·¼ 31ê°œ ë°ì´í„° ì„ íƒë¨ (post-merge)")

        # ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        data_list = []
        for i, (date, row) in enumerate(recent_data.iterrows()):
            try:
                # ì•ˆì „ ì ‘ê·¼: ì»¬ëŸ¼ ë¶€ì¬/ëŒ€ì†Œë¬¸ì ì°¨ì´/NaN ëŒ€ì‘
                def gv(r, k, default=None):
                    return r[k] if (k in r and pd.notna(r[k])) else default
                data_point = {
                    'date': date.strftime('%Y-%m-%d'),
                    'open': round(gv(row, 'Open', 0), 2),
                    'high': round(gv(row, 'High', 0), 2),
                    'low': round(gv(row, 'Low', 0), 2),
                    'close': round(gv(row, 'Close', 0), 2),
                    'volume': int(gv(row, 'Volume', 0) or 0),
                    'ema5': round(gv(row, 'EMA5'), 2) if gv(row, 'EMA5') is not None else None,
                    'ema20': round(gv(row, 'EMA20'), 2) if gv(row, 'EMA20') is not None else None,
                    'ema40': round(gv(row, 'EMA40'), 2) if gv(row, 'EMA40') is not None else None,
                    'macd': round(gv(row, 'MACD'), 4) if gv(row, 'MACD') is not None else None,
                    'macd_signal': round(gv(row, 'MACD_Signal'), 4) if gv(row, 'MACD_Signal') is not None else None,
                    'macd_histogram': round(gv(row, 'MACD_Histogram'), 4) if gv(row, 'MACD_Histogram') is not None else None,
                    'bb_upper': round(gv(row, 'BB_Upper'), 2) if gv(row, 'BB_Upper') is not None else None,
                    'bb_middle': round(gv(row, 'BB_Middle'), 2) if gv(row, 'BB_Middle') is not None else None,
                    'bb_lower': round(gv(row, 'BB_Lower'), 2) if gv(row, 'BB_Lower') is not None else None,
                    'rsi': round(gv(row, 'RSI'), 2) if gv(row, 'RSI') is not None else None,
                    'stoch_k': round(gv(row, 'Stoch_K'), 2) if gv(row, 'Stoch_K') is not None else None,
                    'stoch_d': round(gv(row, 'Stoch_D'), 2) if gv(row, 'Stoch_D') is not None else None,
                    'ichimoku_tenkan': round(gv(row, 'Ichimoku_Tenkan'), 2) if gv(row, 'Ichimoku_Tenkan') is not None else None,
                    'ichimoku_kijun': round(gv(row, 'Ichimoku_Kijun'), 2) if gv(row, 'Ichimoku_Kijun') is not None else None,
                    'ichimoku_senkou_a': round(gv(row, 'Ichimoku_Senkou_A'), 2) if gv(row, 'Ichimoku_Senkou_A') is not None else None,
                    'ichimoku_senkou_b': round(gv(row, 'Ichimoku_Senkou_B'), 2) if gv(row, 'Ichimoku_Senkou_B') is not None else None,
                    'volume_ratio_5d': round(gv(row, 'Volume_Ratio_5d'), 2) if gv(row, 'Volume_Ratio_5d') is not None else None,
                    'volume_ratio_20d': round(gv(row, 'Volume_Ratio_20d'), 2) if gv(row, 'Volume_Ratio_20d') is not None else None,
                    'volume_ratio_40d': round(gv(row, 'Volume_Ratio_40d'), 2) if gv(row, 'Volume_Ratio_40d') is not None else None,
                    'close_gap_ema20': round(gv(row, 'Close_Gap_EMA20'), 2) if gv(row, 'Close_Gap_EMA20') is not None else None,
                    'close_gap_ema40': round(gv(row, 'Close_Gap_EMA40'), 2) if gv(row, 'Close_Gap_EMA40') is not None else None,
                }
                data_list.append(data_point)
                
                # EMA ë°°ì—´ ë¬¸ìì—´ ë³´ê°•: (ê°€ëŠ¥ ì‹œ)
                try:
                    if 'EMA_Array_Order' in row and pd.notna(row['EMA_Array_Order']):
                        data_point['ema_array'] = {'full_array': str(row['EMA_Array_Order'])}
                except Exception:
                    pass

                # ì²« ë²ˆì§¸ í•­ëª©ì˜ í‚¤ë“¤ì„ ë¡œê¹…
                if i == 0:
                    # logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì²« ë²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸ í‚¤ë“¤: {list(data_point.keys())}")
                    # logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì²« ë²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸ ìƒ˜í”Œ: {data_point}")
                    pass
                
            except Exception as row_error:
                logging.error(f"get_ohlcv_with_indicators: [{ticker}] í–‰ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {row_error}")
                logging.error(f"get_ohlcv_with_indicators: [{ticker}] ë¬¸ì œ í–‰ ë°ì´í„°: {row}")
                continue
        
        logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì„±ê³µì ìœ¼ë¡œ {len(data_list)}ê°œ ë°ì´í„° í¬ì¸íŠ¸ ë¡œë“œë¨ ({timeframe})")
        
        # ì§€í‘œ ë°ì´í„°ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if data_list and len(data_list) > 0:
            first_item = data_list[0]
            if 'ema5' in first_item:
                logging.info(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë°ì´í„°ê°€ í¬í•¨ëœ ë°ì´í„° ë°˜í™˜")
            else:
                logging.warning(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œ ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë°ì´í„° ë°˜í™˜")
        
        return data_list
        
    except Exception as e:
        logging.error(f"get_ohlcv_with_indicators: [{ticker}] ì§€í‘œì™€ í•¨ê»˜ OHLCV ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ timeframeë³„ ê¸°ë³¸ OHLCV ë°ì´í„° ë°˜í™˜ (íŠœí”Œ ë°˜í™˜ ê¸ˆì§€)
        daily_data, weekly_data, monthly_data = get_ohlcv_data_for_ai(ticker, market_type)
        if timeframe == 'd':
            return daily_data
        if timeframe == 'w':
            return weekly_data
        if timeframe == 'm':
            return monthly_data
        return daily_data

def format_data_with_indicators_for_prompt(data):
    """ì§€í‘œ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    logging.info(f"format_data_with_indicators_for_prompt: ë°ì´í„° ê°œìˆ˜ {len(data) if data else 0}")
    
    if not data:
        logging.warning("format_data_with_indicators_for_prompt: ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
        return "ë°ì´í„° ì—†ìŒ"
    
    try:
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        if not isinstance(data, list):
            logging.error(f"format_data_with_indicators_for_prompt: ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜. íƒ€ì…: {type(data)}")
            return "ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜"
        
        if len(data) == 0:
            logging.warning("format_data_with_indicators_for_prompt: ë¹ˆ ë¦¬ìŠ¤íŠ¸")
            return "ë°ì´í„° ì—†ìŒ"
        
        # ì²« ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
        first_item = data[0]
        if not isinstance(first_item, dict):
            logging.error(f"format_data_with_indicators_for_prompt: ì²« ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜. íƒ€ì…: {type(first_item)}")
            return "ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜"
        
        logging.info(f"format_data_with_indicators_for_prompt: ì²« ë²ˆì§¸ í•­ëª© í‚¤ë“¤: {list(first_item.keys())}")
        # ë””ë²„ê·¸: ì§€í‘œ í‚¤ ë¯¸ì¡´ì¬ ì‹œ ì›ì¸ ì¶”ì  ë¡œê·¸ ê°•í™”
        try:
            missing_keys = []
            for key in ['ema5','ema20','ema40','macd','macd_signal','macd_histogram','bb_upper','bb_middle','bb_lower','rsi','stoch_k','stoch_d']:
                if key not in first_item:
                    missing_keys.append(key)
            if missing_keys:
                logging.warning(f"format_data_with_indicators_for_prompt: ì§€í‘œ í‚¤ ëˆ„ë½: {missing_keys}")
        except Exception as mk_err:
            logging.warning(f"format_data_with_indicators_for_prompt: ì§€í‘œ í‚¤ ì ê²€ ì‹¤íŒ¨: {mk_err}")
        
        # ì²« ë²ˆì§¸ í•­ëª© í™•ì¸ ë° ë””ë²„ê¹…
        if data and len(data) > 0:
            first_item = data[0]
            logging.info(f"format_data_with_indicators_for_prompt: ì²« ë²ˆì§¸ í•­ëª© í‚¤ë“¤: {list(first_item.keys())}")
            
            # ì§€í‘œ í‚¤ë“¤ í™•ì¸ ë° ë¡œê¹…
            indicator_keys = []
            if 'ema5' in first_item:
                indicator_keys = [
                    'ema5', 'ema20', 'ema40', 'macd', 'macd_signal', 'macd_histogram',
                    'rsi', 'bb_upper', 'bb_middle', 'bb_lower', 'stoch_k', 'stoch_d',
                    'ichimoku_tenkan', 'ichimoku_kijun', 'volume_ratio_5d', 'volume_ratio_20d', 'volume_ratio_40d'
                ]
                logging.info(f"format_data_with_indicators_for_prompt: ì§€í‘œ í‚¤ë“¤ ë°œê²¬: {indicator_keys}")
            else:
                logging.warning("format_data_with_indicators_for_prompt: ema5 í‚¤ê°€ ì—†ìŒ - ì§€í‘œ ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ")
                # ê¸°ë³¸ OHLCV ë°ì´í„°ë§Œ ì²˜ë¦¬
                indicator_keys = []
        
        formatted_lines = []
        
        # í—¤ë” ë¼ì¸ ì¶”ê°€
        if indicator_keys:
            header = "ë‚ ì§œ, ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰ | ì§€í‘œ: " + ", ".join([key.upper() for key in indicator_keys])
            formatted_lines.append(header)
            logging.info(f"format_data_with_indicators_for_prompt: í—¤ë” ì¶”ê°€: {header}")
        else:
            header = "ë‚ ì§œ, ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰"
            formatted_lines.append(header)
            logging.info(f"format_data_with_indicators_for_prompt: ê¸°ë³¸ í—¤ë” ì¶”ê°€: {header}")
        
        # ë°ì´í„° ë¼ì¸ë“¤ ì¶”ê°€
        processed_count = 0
        for i, item in enumerate(data):
            try:
                # ê¸°ë³¸ OHLCV ë°ì´í„° - í‚¤ ì´ë¦„ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬
                open_val = item.get('open') or item.get('Open', 0)
                high_val = item.get('high') or item.get('High', 0)
                low_val = item.get('low') or item.get('Low', 0)
                close_val = item.get('close') or item.get('Close', 0)
                volume_val = item.get('volume') or item.get('Volume', 0)
                date_val = item.get('date') or item.get('Date', '')
                
                # numpy.float64 íƒ€ì… ì²˜ë¦¬
                if hasattr(open_val, 'item'):
                    open_val = open_val.item()
                if hasattr(high_val, 'item'):
                    high_val = high_val.item()
                if hasattr(low_val, 'item'):
                    low_val = low_val.item()
                if hasattr(close_val, 'item'):
                    close_val = close_val.item()
                if hasattr(volume_val, 'item'):
                    volume_val = volume_val.item()
                
                # float ë³€í™˜ ë° ì•ˆì „í•œ ì²˜ë¦¬
                try:
                    open_val = float(open_val) if open_val is not None else 0
                    high_val = float(high_val) if high_val is not None else 0
                    low_val = float(low_val) if low_val is not None else 0
                    close_val = float(close_val) if close_val is not None else 0
                    volume_val = int(volume_val) if volume_val is not None else 0
                except (ValueError, TypeError) as conv_error:
                    logging.error(f"format_data_with_indicators_for_prompt: ê°’ ë³€í™˜ ì˜¤ë¥˜ (í•­ëª© {i}): {conv_error}")
                    open_val = high_val = low_val = close_val = volume_val = 0
                
                line = f"{date_val}, {open_val:.2f}, {high_val:.2f}, {low_val:.2f}, {close_val:.2f}, {volume_val}"
                
                # ì§€í‘œ ë°ì´í„° ì¶”ê°€
                if indicator_keys:
                    indicator_values = []
                    for key in indicator_keys:
                        value = item.get(key)
                        if value is not None and not pd.isna(value):
                            # numpy.float64 íƒ€ì… ì²˜ë¦¬
                            if hasattr(value, 'item'):
                                value = value.item()
                            try:
                                indicator_values.append(f"{float(value):.4f}")
                            except (ValueError, TypeError) as ind_error:
                                logging.error(f"format_data_with_indicators_for_prompt: ì§€í‘œ ê°’ ë³€í™˜ ì˜¤ë¥˜ (í‚¤ {key}, ê°’ {value}): {ind_error}")
                                indicator_values.append("")
                        else:
                            indicator_values.append("")
                    line += " | " + ", ".join(indicator_values)
                
                formatted_lines.append(line)
                processed_count += 1
                
            except Exception as item_error:
                logging.error(f"format_data_with_indicators_for_prompt: í•­ëª© {i} í¬ë§·íŒ… ì˜¤ë¥˜: {item_error}")
                logging.error(f"format_data_with_indicators_for_prompt: ë¬¸ì œ í•­ëª© ë°ì´í„°: {item}")
                continue
        
        result = '\n'.join(formatted_lines)
        logging.info(f"format_data_with_indicators_for_prompt: ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ í•­ëª© ìˆ˜: {processed_count}/{len(data)}")
        logging.info(f"format_data_with_indicators_for_prompt: ê²°ê³¼ ê¸¸ì´: {len(result)} ë¬¸ì")
        
        return result
        
    except Exception as e:
        logging.error(f"format_data_with_indicators_for_prompt: ì „ì²´ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
        return "ë°ì´í„° í¬ë§·íŒ… ì˜¤ë¥˜"

def load_ai_prompt_template():
    """AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ ai_analysis_prompt.txt íŒŒì¼ì„ ì‚¬ìš©
        prompt_path = os.path.join(os.path.dirname(__file__), '..', 'ai_analysis_prompt.txt')
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logging.error(f"Error loading prompt template: {e}")
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
        return """ë°‘ì— ì œê³µë˜ëŠ” {ticker} ì¢…ëª©ì˜ OHLCV ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ EMA, MACD, ë³¼ë¦°ì €ë°´ë“œ, ì¼ëª©ê· í˜•í‘œ, RSI, ìŠ¤í† ìºìŠ¤í‹± ì˜¤ì‹¤ë ˆì´ì…˜ ë“± ê°€ìš©í•œ ê¸°ìˆ ì  ì§€í‘œë“¤ì„ ê³„ì‚°í•˜ì—¬ ì°¨íŠ¸ë¥¼ ë¶„ì„ í›„, ìŠ¤ìœ™ íˆ¬ììì˜ ë§¤ìˆ˜ ë° ë§¤ë„ íƒ€ì´ë°ì— ëŒ€í•œ ì˜ê²¬ì„ ê°œì§„í•´ë¼. ì´ ë¶„ì„ì˜ í•µì‹¬ë‚´ìš©ì„ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì œì¼ ì²«ë¨¸ë¦¬ì— **í•µì‹¬ ìš”ì•½**ì˜ ì œëª©ì•„ë˜ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì œì‹œí•´ë¼.ìƒì„¸ë¶„ì„ì€ ì¼ë´‰-ì£¼ë´‰-ì›”ë´‰ì˜ ìˆœìœ¼ë¡œ ë°°ì—´í•´ë¼.

ì¼ë´‰ ë°ì´í„° (ìµœê·¼ 30ì¼):
ë‚ ì§œ, ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰
{daily_ohlcv_data}

ì£¼ë´‰ ë°ì´í„° (ìµœê·¼ 30ì£¼):
{weekly_ohlcv_data}

ì›”ë´‰ ë°ì´í„° (ìµœê·¼ 30ê°œì›”):
{monthly_ohlcv_data}"""

def format_data_for_prompt(data):
    """ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    if not data:
        return "ë°ì´í„° ì—†ìŒ"
    
    # ìµœê·¼ ë°ì´í„°ë§Œ ì„ íƒ (ì¼ë´‰: 30ì¼, ì£¼ë´‰: 30ì£¼, ì›”ë´‰: 30ê°œì›”)
    if len(data) > 30:
        data = data[-30:]  # ìµœê·¼ 30ê°œë§Œ ì„ íƒ
    
    formatted = "ë‚ ì§œ, ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰\n"
    for item in data:
        formatted += f"{item['date']}, {item['open']}, {item['high']}, {item['low']}, {item['close']}, {item['volume']}\n"
    
    return formatted



@analysis_bp.route('/<ticker>')
@login_required
def analysis_page(ticker):
    """ê°œë³„ ì¢…ëª© ë¶„ì„ í˜ì´ì§€"""
    stock = Stock.query.filter_by(ticker=ticker).first_or_404()
    
    cache_service = CacheService()
    unified_service = UnifiedMarketAnalysisService(cache_service)
    
    # ì¢…í•© ë¶„ì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    analysis_data = unified_service.analyze_stock_comprehensive(stock.ticker, stock.market_type)
    
    return render_template('analysis/analysis_page.html', stock=stock, analysis_data=analysis_data)

@analysis_bp.route('/analysis/<ticker>')
@login_required
def analyze_ticker(ticker):
    """ìƒì„¸ë¶„ì„ ë²„íŠ¼ì„ ìœ„í•œ ë¼ìš°íŠ¸ - ì‹œì¥ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ AI ë¶„ì„ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    try:
        ticker = ticker.upper()
        
        # ì‹œì¥ íƒ€ì… ê°ì§€ (íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¡œ íŒë‹¨)
        kospi_data_dir = os.path.join("static/data", "KOSPI")
        kosdaq_data_dir = os.path.join("static/data", "KOSDAQ")
        us_data_dir = os.path.join("static/data", "US")
        
        market_type = 'KOSPI'  # ê¸°ë³¸ê°’
        
        # KOSPI, KOSDAQ, US ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ í‹°ì»¤ì˜ ë°ì´í„° íŒŒì¼ í™•ì¸
        if os.path.exists(kospi_data_dir):
            kospi_files = [f for f in os.listdir(kospi_data_dir) if f.startswith(f"{ticker}_")]
            if kospi_files:
                market_type = 'KOSPI'
            else:
                # KOSDAQ ë””ë ‰í† ë¦¬ í™•ì¸
                if os.path.exists(kosdaq_data_dir):
                    kosdaq_files = [f for f in os.listdir(kosdaq_data_dir) if f.startswith(f"{ticker}_")]
                    if kosdaq_files:
                        market_type = 'KOSDAQ'
                    else:
                        # US ë””ë ‰í† ë¦¬ í™•ì¸
                        if os.path.exists(us_data_dir):
                            us_files = [f for f in os.listdir(us_data_dir) if f.startswith(f"{ticker}_")]
                            if us_files:
                                market_type = 'US'
        
        # AI ë¶„ì„ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        return redirect(url_for('analysis.ai_analysis', ticker=ticker, market=market_type))
        
    except Exception as e:
        logging.error(f"Analyze ticker error for {ticker}: {e}")
        return f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}", 500

@analysis_bp.route('/ai_analysis/<ticker>/<market>')
@login_required
def ai_analysis(ticker, market):
    """AI ë¶„ì„ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    try:
        ticker = ticker.upper()
        market = market.upper()  # ëŒ€ë¬¸ìë¡œ ë³€í™˜ ì¶”ê°€
        logging.info(f"AI ë¶„ì„ ì‹œì‘: {ticker} ({market})")

        # 0) ì‹œì¥ canonicalization (íŒŒì¼ ì¡´ì¬ ê¸°ë°˜) ë° URL ì •í•©ì„± í™•ë³´
        canonical_market = _detect_canonical_market_by_files(ticker, market)
        if canonical_market != market:
            logging.info(f"[CANON] Redirecting to canonical market: {ticker} {market} â†’ {canonical_market}")
            return redirect(url_for('analysis.ai_analysis', ticker=ticker, market=canonical_market))

        # 1) ìºì‹œ ì‹ ì„ ë„ íŒì • í›„ ì¦‰ì‹œ ë°˜í™˜ ì‹œë„ (ì°¨íŠ¸ ìƒì„± ì „ì— ìˆ˜í–‰)
        try:
            cached_html = _try_return_cached_analysis_html(ticker, market)
        except Exception:
            cached_html = None
        if cached_html:
            return cached_html

        # 0-LEGACY) ê¸°ì¡´ ì°¨íŠ¸ ê¸°ë°˜ ì‹œì¥ ìë™ êµì • ë¡œì§ (ì„±ëŠ¥ ë¬¸ì œë¡œ ë¹„í™œì„±í™”) â€” ë³´ì¡´ìš© ì£¼ì„
        # resolved_market = market
        # charts, chart_error = generate_charts(ticker, resolved_market)
        # if chart_error:
        #     logging.info(f"[AUTO-MARKET] Primary market failed for {ticker}/{resolved_market}: {chart_error}")
        #     market_candidates = ['US', 'KOSPI', 'KOSDAQ']
        #     if resolved_market in market_candidates:
        #         market_candidates.remove(resolved_market)
        #         market_candidates.insert(0, resolved_market)
        #     for cand in market_candidates:
        #         try:
        #             data_dir = os.path.join("static", "data", cand)
        #             pattern = os.path.join(data_dir, f"{ticker}_ohlcv_d_*.csv")
        #             if glob.glob(pattern):
        #                 resolved_market = cand
        #                 break
        #         except Exception:
        #             continue
        #     if resolved_market != market:
        #         logging.info(f"[AUTO-MARKET] Resolved market for {ticker}: {resolved_market} (was {market})")
        #         market = resolved_market
        #         charts, chart_error = generate_charts(ticker, market)

        # 2) ì—¬ê¸°ì„œë¶€í„°ë§Œ ì°¨íŠ¸ ìƒì„± ë“± ë¹„ìš© ì‘ì—… ìˆ˜í–‰
        charts, chart_error = generate_charts(ticker, market)

        # ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸°
        from models import Stock
        stock = Stock.query.filter_by(ticker=ticker).first()
        company_name = stock.company_name if stock and stock.company_name else ticker
        
        # [2025-08-09 ìˆ˜ì • ì „ ì½”ë“œ - ì£¼ì„ ì²˜ë¦¬]
        # OHLCVë§Œ ë¡œë”© (ì§€í‘œ ì—†ìŒ) - AI í”„ë¡¬í”„íŠ¸ ë°ì´í„° ëˆ„ë½ì˜ ì›ì¸
        # daily_data, weekly_data, monthly_data = get_ohlcv_data_for_ai(ticker, market)
        
        # [2025-08-09 ìˆ˜ì •] OHLCV + ì§€í‘œ í†µí•© ë°ì´í„° ë¡œë”© (AI í”„ë¡¬í”„íŠ¸ìš©)
        daily_data = get_ohlcv_with_indicators(ticker, 'd', market)
        weekly_data = get_ohlcv_with_indicators(ticker, 'w', market)
        monthly_data = get_ohlcv_with_indicators(ticker, 'm', market)

        # ë°ì´í„° ì‹¤íŒ¨ ì‹œ í•œ ë²ˆ ë” ì‹œì¥ êµì • ì¬ì‹œë„
        if (not daily_data) and (not weekly_data) and (not monthly_data):
            logging.info(f"[AUTO-MARKET] Indicator data empty for {ticker}/{market} â†’ probing other markets")
            for cand in ['US', 'KOSPI', 'KOSDAQ']:
                if cand == market:
                    continue
                try:
                    data_dir = os.path.join("static", "data", cand)
                    if glob.glob(os.path.join(data_dir, f"{ticker}_indicators_d_*.csv")) or \
                       glob.glob(os.path.join(data_dir, f"{ticker}_ohlcv_d_*.csv")):
                        market = cand
                        logging.info(f"[AUTO-MARKET] Resolved by indicators/ohlcv presence: {ticker} â†’ {market}")
                        # ì¬ë¡œë“œ
                        charts, chart_error = generate_charts(ticker, market)
                        daily_data = get_ohlcv_with_indicators(ticker, 'd', market)
                        weekly_data = get_ohlcv_with_indicators(ticker, 'w', market)
                        monthly_data = get_ohlcv_with_indicators(ticker, 'm', market)
                        break
                except Exception:
                    continue
        
        # ë°ì´í„° ë¶€ì¬ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ/ë³´ì¥ì€ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
        # ì‚¬ìš©ì ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ëŒ€ì²´: ì–´ë“œë¯¼ í™ˆ ìƒˆë¡œê³ ì¹¨ ìœ ë„
        analysis_result = None
        analysis_error = None
        if (not daily_data) and (not weekly_data) and (not monthly_data):
            analysis_error = "ì§€í‘œ íŒŒì¼ì€ ì¡´ì¬í•˜ì§€ë§Œ ì½ê¸° ì‹¤íŒ¨ë¡œ ë³´ì…ë‹ˆë‹¤. ê´€ë¦¬ì í™ˆì„ ìƒˆë¡œê³ ì¹¨í•´ ë°ì´í„°ë¥¼ ì¬ìƒì„±í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
        # ì§€í‘œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° - get_stock_with_indicators í•¨ìˆ˜ ì‚¬ìš© (ì–´ë“œë¯¼ í™ˆê³¼ ë™ì¼í•œ êµ¬ì¡°)
        # [2025-08-09 ìˆ˜ì • ì „ ì½”ë“œ - ì£¼ì„ ì²˜ë¦¬]
        # stock_data = get_stock_with_indicators(ticker, market)
        # indicators_data = None
        # if stock_data:
        #     indicators_data = {ticker: stock_data}
        # ì„ì‹œë¡œ ë¹ˆ ë°ì´í„° ì‚¬ìš© - ì´ ë¶€ë¶„ì´ í˜ì´ì§€ í‘œì‹œ ì‹¤íŒ¨ì˜ ì›ì¸ì´ì—ˆìŒ
        # indicators_data = None
        
        # [2025-08-09 ìˆ˜ì •] admin_homeê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì§€í‘œ ë°ì´í„° ë¡œë”©
        try:
            from services.technical_indicators_service import TechnicalIndicatorsService
            
            technical_service = TechnicalIndicatorsService()
            indicators_df = technical_service.read_indicators_csv(ticker, market, 'd')
            
            # None ì²´í¬ ë° DataFrame ê²€ì¦ ê°•í™”
            if indicators_df is None:
                indicators_data = None
                logging.warning(f"[{ticker}] í˜ì´ì§€ìš© ì§€í‘œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ - None ë°˜í™˜ë¨")
            elif not isinstance(indicators_df, pd.DataFrame):
                indicators_data = None
                logging.warning(f"[{ticker}] í˜ì´ì§€ìš© ì§€í‘œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ - DataFrameì´ ì•„ë‹˜: {type(indicators_df)}")
            elif indicators_df.empty:
                indicators_data = None
                logging.warning(f"[{ticker}] í˜ì´ì§€ìš© ì§€í‘œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ - DataFrameì´ ë¹„ì–´ìˆìŒ")
            else:
                latest_indicators = indicators_df.iloc[-1]
                
                # [ë©”ëª¨] 2025-08-19: ë“±ë½ë¥  ìµœì‹ ì¹˜ ì¡°íšŒ ê²½ë¡œ ë‹¨ì¼í™”
                # ê¸°ì¡´ CSV ì»¬ëŸ¼ ì§ì ‘ ì°¸ì¡°ëŠ” ë³´ì¡´í•˜ë˜, í‘œì¤€ í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ë¥¼ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
                try:
                    change_pct_value = technical_service.get_latest_change_percent(ticker, 'd', market)
                except Exception:
                    change_pct_value = latest_indicators.get('Change_Percent', 0)

                indicators_data = {ticker: {
                    'indicators': {
                        'ema5': latest_indicators.get('EMA5', 0),
                        'ema20': latest_indicators.get('EMA20', 0),
                        'ema40': latest_indicators.get('EMA40', 0),
                        'macd_line': latest_indicators.get('MACD', 0),
                        'macd_signal': latest_indicators.get('MACD_Signal', 0),
                        'macd_histogram': latest_indicators.get('MACD_Histogram', 0),
                        'rsi': latest_indicators.get('RSI', 0),
                        'stoch_k': latest_indicators.get('Stoch_K', 0),
                        'stoch_d': latest_indicators.get('Stoch_D', 0),
                        'volume_ratio_5d': latest_indicators.get('Volume_Ratio_5d', 0),
                        'volume_ratio_20d': latest_indicators.get('Volume_Ratio_20d', 0),
                        'volume_ratio_40d': latest_indicators.get('Volume_Ratio_40d', 0),
                        'change_percent': change_pct_value
                    },
                    'analysis': {
                        'macd_signals': {},  # í¬ë¡œìŠ¤ì˜¤ë²„ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                        'ema_signals': {},   # í¬ë¡œìŠ¤ì˜¤ë²„ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                        'gap_ema20': 0,
                        'gap_ema40': 0,
                        'ema_array': {'full_array': ''}
                    }
                }}
                
                # CrossInfo CSV ì½ê¸° ë° ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ ì¶”ê°€ (admin_homeê³¼ ë™ì¼í•œ ë°©ì‹)
                try:
                    from services.market.data_reading_service import DataReadingService
                    data_reading_service = DataReadingService()
                    # CrossInfo í˜¸ì¶œ ì¸ì ì •ë¦¬ (caller ì¸ì ì œê±°)
                    crossinfo_df = data_reading_service.read_crossinfo_csv(ticker, market)
                    
                    if not crossinfo_df.empty:
                        latest_crossinfo = crossinfo_df.iloc[-1]

                    # CrossInfo ì»¬ëŸ¼ëª… ìë™ ë§¤í•‘(í‘œì¤€í™”)
                    import re
                    def _norm(s: str) -> str:
                        return re.sub(r"[^a-z0-9]", "", str(s).lower())
                    aliases = {
                        'Close_Gap_EMA20': ['Close_Gap_EMA20','CloseGapEMA20','close_gap_ema20','Close-EMA20-Gap','Gap_EMA20','GapEMA20','Close_EMA20_Gap'],
                        'Close_Gap_EMA40': ['Close_Gap_EMA40','CloseGapEMA40','close_gap_ema40','Close-EMA40-Gap','Gap_EMA40','GapEMA40','Close_EMA40_Gap'],
                        'EMA_Array_Order': ['EMA_Array_Order','EMAArrayOrder','ema_array_order','EMA_Order','EMA Array Order','EmaArrayOrder']
                    }
                    norm_map = {k: {_norm(a) for a in v} for k, v in aliases.items()}
                    resolved = {}
                    for col in list(crossinfo_df.columns):
                        n = _norm(col)
                        for tgt, aset in norm_map.items():
                            if n in aset and tgt not in resolved:
                                resolved[tgt] = col
                                break
                        
                        # MACD ê·¼ì ‘ì„± ì •ë³´ íŒŒì‹± í•¨ìˆ˜
                        def parse_macd_proximity(proximity_value):
                            """MACD ê·¼ì ‘ì„± ì •ë³´ë¥¼ íŒŒì‹±í•˜ì—¬ type ê°’ë§Œ ë°˜í™˜"""
                            if not proximity_value or proximity_value == 'no_proximity':
                                return 'no_proximity'
                            
                            # ë”•ì…”ë„ˆë¦¬ ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                            if isinstance(proximity_value, str) and proximity_value.startswith('{'):
                                try:
                                    import ast
                                    proximity_dict = ast.literal_eval(proximity_value)
                                    if isinstance(proximity_dict, dict):
                                        return proximity_dict.get('type', 'no_proximity')
                                except:
                                    pass
                            
                            # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
                            return str(proximity_value)
                        
                        # ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ ì¶”ê°€ (admin_homeê³¼ ë™ì¼)
                        # í‘œì¤€ í‚¤ ì¶”ì¶œ
                        gap20_val = latest_crossinfo.get(resolved.get('Close_Gap_EMA20', 'Close_Gap_EMA20'), 0.0)
                        gap40_val = latest_crossinfo.get(resolved.get('Close_Gap_EMA40', 'Close_Gap_EMA40'), 0.0)
                        order_val = latest_crossinfo.get(resolved.get('EMA_Array_Order', 'EMA_Array_Order'), '')

                        indicators_data[ticker]['analysis'].update({
                            'ema_array_pattern': latest_crossinfo.get('EMA_Array_Pattern', 'ë¶„ì„ë¶ˆê°€'),
                            'ema_array_order': order_val if order_val != '' else 'ë¶„ì„ë¶ˆê°€',
                            'close_gap_ema20': gap20_val,
                            'close_gap_ema40': gap40_val,
                            
                            # MACD í¬ë¡œìŠ¤ì˜¤ë²„ ì •ë³´
                            'macd_signals': {
                                'type': 'crossover' if latest_crossinfo.get('MACD_Latest_Crossover_Type') else 'normal',
                                'status': latest_crossinfo.get('MACD_Latest_Crossover_Type', 'ì •ìƒ'),
                                'latest_crossover_type': latest_crossinfo.get('MACD_Latest_Crossover_Type'),
                                'latest_crossover_date': latest_crossinfo.get('MACD_Latest_Crossover_Date'),
                                'days_since_crossover': latest_crossinfo.get('MACD_Days_Since_Crossover'),
                                'proximity_type': parse_macd_proximity(latest_crossinfo.get('MACD_Current_Proximity'))
                            },
                            
                            # EMA í¬ë¡œìŠ¤ì˜¤ë²„ ì •ë³´
                            'ema_signals': {
                                'type': 'crossover' if latest_crossinfo.get('EMA_Latest_Crossover_Type') else 'normal',
                                'status': latest_crossinfo.get('EMA_Latest_Crossover_Type', 'ì •ìƒ'),
                                'latest_crossover_type': latest_crossinfo.get('EMA_Latest_Crossover_Type'),
                                'latest_crossover_date': latest_crossinfo.get('EMA_Latest_Crossover_Date'),
                                'days_since_crossover': latest_crossinfo.get('EMA_Days_Since_Crossover'),
                                'crossover_pair': latest_crossinfo.get('EMA_Crossover_Pair'),
                                'proximity_type': latest_crossinfo.get('EMA_Current_Proximity', 'no_proximity')
                            }
                        })
                        # í…œí”Œë¦¿ í˜¸í™˜: ema_array.full_array ì±„ìš°ê¸°
                        try:
                            indicators_data[ticker]['analysis']['ema_array'] = {'full_array': str(order_val) if order_val != '' else ''}
                        except Exception:
                            pass
                        logging.debug(f"[{ticker}] CrossInfo CSV ì½ê¸° ì„±ê³µ - EMA ë°°ì—´(Order): {order_val}")
                except Exception as e:
                    logging.warning(f"[{ticker}] CrossInfo CSV ì½ê¸° ì‹¤íŒ¨: {e}")
                
                logging.info(f"[{ticker}] í˜ì´ì§€ìš© ì§€í‘œ ë°ì´í„° ë¡œë”© ì„±ê³µ (admin_homeê³¼ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)")
                
        except Exception as e:
            logging.error(f"[{ticker}] ì§€í‘œ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
            indicators_data = None
        
        logging.info(f"ai_analysis: ì§€í‘œ ë°ì´í„° ê²°ê³¼ - {indicators_data}")
        
        # [2025-08-09 ìˆ˜ì • ì „ ì½”ë“œ - ì£¼ì„ ì²˜ë¦¬]
        # AI ë¶„ì„ ìˆ˜í–‰ (ì „ë‹¬ë°›ì€ ë°ì´í„° ë¬´ì‹œë˜ê³  ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ë¡œë”©í•˜ëŠ” ë¬¸ì œ)
        # analysis_result, analysis_error = perform_ai_analysis(ticker, daily_data, weekly_data, monthly_data, market, company_name)
        
        # [2025-08-09 ìˆ˜ì •] ì§€í‘œ í¬í•¨ ë°ì´í„°ë¡œ AI ë¶„ì„ ìˆ˜í–‰ (ë°ì´í„° ì—†ëŠ” ê²½ìš° ìŠ¤í‚µí•˜ê³  ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ)
        if analysis_error is None:
            analysis_result, analysis_error = perform_ai_analysis_with_data(
                ticker, daily_data, weekly_data, monthly_data, market, company_name
            )
        
        # HTML ë Œë”ë§
        html_content = render_template('analysis/ai_analysis.html',
                             ticker=ticker,
                             company_name=company_name,
                             market=market,
                             charts=charts,
                             chart_error=chart_error,
                             analysis_result=analysis_result,
                             analysis_error=analysis_error,
                             indicators_data=indicators_data,
                             daily_data=daily_data,
                             weekly_data=weekly_data,
                             monthly_data=monthly_data)
        
        # static/analysis í´ë”ì— HTML íŒŒì¼ ìë™ ì €ì¥
        try:
            analysis_dir = os.path.join("static", "analysis")
            os.makedirs(analysis_dir, exist_ok=True)
            
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{ticker}_AI_Analysis_{market}_{timestamp}.html"
            filepath = os.path.join(analysis_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logging.info(f"AI analysis HTML saved: {filepath}")
        except Exception as save_error:
            logging.error(f"Failed to save HTML file: {save_error}")
        
        return html_content
        
    except Exception as e:
        logging.error(f"AI analysis page error: {e}")
        return render_template('analysis/ai_analysis.html',
                             ticker=ticker,
                             market=market,
                             error=str(e))

@analysis_bp.route('/view_chart/<ticker>')
@login_required
def view_chart(ticker):
    """ì°¨íŠ¸ ë³´ê¸° - ì €ì¥ëœ indicator ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        ticker = ticker.upper()
        
        # ì‹œì¥ íƒ€ì… ê°ì§€
        kospi_data_dir = os.path.join("static/data", "KOSPI")
        kosdaq_data_dir = os.path.join("static/data", "KOSDAQ")
        us_data_dir = os.path.join("static/data", "US")
        
        market_type = 'KOSPI'  # ê¸°ë³¸ê°’
        
        # KOSPI, KOSDAQ, US ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ í‹°ì»¤ì˜ ë°ì´í„° íŒŒì¼ í™•ì¸
        if os.path.exists(kospi_data_dir):
            kospi_files = [f for f in os.listdir(kospi_data_dir) if f.startswith(f"{ticker}_")]
            if kospi_files:
                market_type = 'KOSPI'
            else:
                # KOSDAQ ë””ë ‰í† ë¦¬ í™•ì¸
                if os.path.exists(kosdaq_data_dir):
                    kosdaq_files = [f for f in os.listdir(kosdaq_data_dir) if f.startswith(f"{ticker}_")]
                    if kosdaq_files:
                        market_type = 'KOSDAQ'
                    else:
                        # US ë””ë ‰í† ë¦¬ í™•ì¸
                        if os.path.exists(us_data_dir):
                            us_files = [f for f in os.listdir(us_data_dir) if f.startswith(f"{ticker}_")]
                            if us_files:
                                market_type = 'US'
        
        # ì°¨íŠ¸ ìƒì„±
        charts, chart_error = generate_charts(ticker, market_type)
        
        if chart_error:
            return f"ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {chart_error}", 500
        
        return render_template('analysis/chart_view.html',
                             ticker=ticker,
                             charts=charts)
        
    except Exception as e:
        logging.error(f"Chart view error for {ticker}: {e}")
        return f"ì°¨íŠ¸ ë³´ê¸° ì˜¤ë¥˜: {str(e)}", 500 

@analysis_bp.route('/api/indicators/<ticker>/<market_type>/<timeframe>')
def get_indicators_api(ticker, market_type, timeframe):
    """íŠ¹ì • ì¢…ëª©ì˜ ì§€í‘œ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜"""
    try:
        technical_service = TechnicalIndicatorsService()
        indicators_df = technical_service.read_indicators_csv(ticker, timeframe, market_type)
        
        if indicators_df.empty:
            return jsonify({'error': 'ì§€í‘œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
            
        return jsonify(indicators_df.to_dict(orient='records'))
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@analysis_bp.route('/api/crossinfo/<ticker>/<market_type>')
def get_crossinfo_api(ticker, market_type):
    """íŠ¹ì • ì¢…ëª©ì˜ CrossInfo ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜"""
    try:
        data_reading_service = DataReadingService()
        crossinfo_df = data_reading_service.read_crossinfo_csv(ticker, market_type)

        if crossinfo_df.empty:
            return jsonify({'error': 'CrossInfo ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404

        return jsonify(crossinfo_df.to_dict(orient='records'))
    except Exception as e:
        return jsonify({'error': str(e)}), 500 